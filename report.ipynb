{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multiclass Classification Algorithm with Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression Pseudocode\n",
    "\n",
    "\n",
    "Initialize weights $W$ as a vector of small random values or zeros\n",
    "Initialize bias $b$ as a small random value or zero\n",
    "\n",
    "\n",
    "function sigmoid(z):\n",
    "   return 1 / (1 + exp(-z))\n",
    "\n",
    "\n",
    "For each epoch (iteration over the dataset):\n",
    "   z = W * X + b \n",
    "\n",
    "\n",
    "   y_hat = sigmoid(z)\n",
    "   loss = - (1 / m) * sum(y * log(y_hat) + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "\n",
    "   dW = (1 / m) * X.T * (y_hat - y)  # Gradient of loss with respect to W\n",
    "   db = (1 / m) * sum(y_hat - y)     # Gradient of loss with respect to b\n",
    "\n",
    "\n",
    "   W = W - learning_rate * dW\n",
    "   b = b - learning_rate * db\n",
    "\n",
    "\n",
    "# Prediction after Training\n",
    "function predict(X_new):\n",
    "   z_new = W * X_new + b\n",
    "   y_new_hat = sigmoid(z_new)\n",
    "\n",
    "\n",
    "   if y_new_hat >= 0.5:\n",
    "       return 1\n",
    "   else:\n",
    "       return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-Pairs Logistic Regression Approach\n",
    "\n",
    "In the all-pairs approach for multi-class classification, multiple logistic regression models are trained for each pair of classes. Hereâ€™s the math involved:\n",
    "\n",
    "The total number of unique pairs of classes for $K$ classes is:\n",
    "\n",
    "$\\text{Number of pairs} = \\binom{K}{2} = \\frac{K(K - 1)}{2}$\n",
    "\n",
    "For each pair of classes $(C_i, C_j)$, we build a binary logistic regression model to distinguish between data points in $C_i$ and $C_j$.\n",
    "\n",
    "### Probability Estimation\n",
    "The probability that a data point $x$ belongs to class $C_i$ rather than $C_j$ is given by:\n",
    "\n",
    "$P(y = 1 | x; \\theta^{(i, j)}) = \\frac{1}{1 + e^{-\\theta^{(i, j) \\top} x}}$\n",
    "\n",
    "where $ \\theta^{(i, j)}$ is the parameter vector specific to the classifier for classes $C_i$ and $C_j$.\n",
    "\n",
    "### Loss Function and Optimization\n",
    "To learn $\\theta^{(i, j)}$, we minimize the binary cross-entropy loss for the data points belonging to classes $C_i$ and $C_j$, denoted $X^{(i, j)}$:\n",
    "\n",
    "$\\mathcal{L}^{(i, j)}(\\theta^{(i, j)}) = - \\sum_{k \\in X^{(i, j)}} \\left( y_k^{(i, j)} \\log P(y = 1 | x_k; \\theta^{(i, j)}) + (1 - y_k^{(i, j)}) \\log P(y = 0 | x_k; \\theta^{(i, j)}) \\right)$\n",
    "\n",
    "We use gradient-based optimization (e.g., gradient descent) to minimize this loss function for each pair.\n",
    "\n",
    "### Pseudocode\n",
    "input:  \n",
    "\n",
    "    training set $S = (x_1,y_1),...,(x_m,y_m)$  \n",
    "\n",
    "    algorithm for binary classification A   \n",
    "\n",
    "for each $i,j \\in \\mathcal{Y} s.t. i < j$  \n",
    "\n",
    "    initialize $S_{i,j}$ to be the empty sequence  \n",
    "\n",
    "    for t = 1,...,m  \n",
    "\n",
    "        If $y_t = i$ add $(x_t,1)$ to $S_{ij}$  \n",
    "\n",
    "        If $y_t = j$ add $(x_t,-1)$ to $S_{ij}$  \n",
    "\n",
    "    Let $h_{i,j} = A(S_{ij})$  \n",
    "\n",
    "output:  \n",
    "\n",
    "    the multiclass hypothesis defined by $h(x) \\in argmax_{i \\in \\mathcal{Y}}(\\sum_{j \\in \\mathcal{Y}} sign(j-i) h_{i,j}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression Math\n",
    "Logistic Regression uses the sigmoid function, which is defined as follows:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where z is $\\langle w,x \\rangle$\n",
    "This function takes in values of $X = \\mathbb{R}^d$ and outputs continuous values in [0,1] that correspond to probabilities that are used to classify the points as $Y$ = {1, -1}.\n",
    "\n",
    "The decision boundary based on this classifier is still $\\langle w,x \\rangle = 0$ and corresponds to a probability of 50%.\n",
    "\n",
    "Now moving on to the loss for logisitic regression, in the binary case, log loss is as follows:\n",
    "$$\n",
    "\\ell(h_{\\mathbf{w}}, (\\mathbf{x}, y)) = \\log(1 + \\exp(-y \\langle \\mathbf{w}, \\mathbf{x} \\rangle))\n",
    "$$\n",
    "\n",
    "This loss function penalizes the degree of wrongness in the case of misclassification.\n",
    "\n",
    "Log loss is also convex, which moves us onto the optimization of the loss function. The optimization is done according to empirical risk minimization, which aims to find the hypothesis within the hypothesis class that minimizes the expected loss over all available data. In other words, ERM selects the hypothesis that produces the lowest average loss on the entire dataset. Since log-loss is convex, it is known that there is at most one global minimum, which would be where the loss is the smallest. \n",
    "In order to find this minimima, the gradients of $L(w)$ are computed with respect to each weight $w_j$. This method called gradient descent is used to iteratively minimize the loss function by adjusting the model parameters in the direction that reduces loss. Specifically, the weights are updated iteratively as follows:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$\\alpha$ in this equation is the learning rate, which controls the size of the steps taken during gradient descent to update model parameters. It is important to select this parameter carefully because an overy large $\\alpha$ can cause the model to overshoot the optimal values, while an overly small $\\alpha$ can result in slow convergence or getting stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
