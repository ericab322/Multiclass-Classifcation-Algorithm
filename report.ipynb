{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multiclass Classification Algorithm with Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression Pseudocode\n",
    "\n",
    "\n",
    "Initialize weights $W$ as a vector of small random values or zeros\n",
    "Initialize bias $b$ as a small random value or zero\n",
    "\n",
    "\n",
    "function sigmoid(z):\n",
    "   return 1 / (1 + exp(-z))\n",
    "\n",
    "\n",
    "For each epoch (iteration over the dataset):\n",
    "   z = W * X + b \n",
    "\n",
    "\n",
    "   y_hat = sigmoid(z)\n",
    "   loss = - (1 / m) * sum(y * log(y_hat) + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "\n",
    "   dW = (1 / m) * X.T * (y_hat - y)  # Gradient of loss with respect to W\n",
    "   db = (1 / m) * sum(y_hat - y)     # Gradient of loss with respect to b\n",
    "\n",
    "\n",
    "   W = W - learning_rate * dW\n",
    "   b = b - learning_rate * db\n",
    "\n",
    "\n",
    "# Prediction after Training\n",
    "function predict(X_new):\n",
    "   z_new = W * X_new + b\n",
    "   y_new_hat = sigmoid(z_new)\n",
    "\n",
    "\n",
    "   if y_new_hat >= 0.5:\n",
    "       return 1\n",
    "   else:\n",
    "       return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-Pairs Logistic Regression Approach\n",
    "\n",
    "In the all-pairs approach for multi-class classification, multiple logistic regression models are trained for each pair of classes. Hereâ€™s the math involved:\n",
    "\n",
    "The total number of unique pairs of classes for $K$ classes is:\n",
    "\n",
    "$\\text{Number of pairs} = \\binom{K}{2} = \\frac{K(K - 1)}{2}$\n",
    "\n",
    "For each pair of classes $(C_i, C_j)$, we build a binary logistic regression model to distinguish between data points in $C_i$ and $C_j$.\n",
    "\n",
    "### Probability Estimation\n",
    "The probability that a data point $x$ belongs to class $C_i$ rather than $C_j$ is given by:\n",
    "\n",
    "$P(y = 1 | x; \\theta^{(i, j)}) = \\frac{1}{1 + e^{-\\theta^{(i, j) \\top} x}}$\n",
    "\n",
    "where $ \\theta^{(i, j)}$ is the parameter vector specific to the classifier for classes $C_i$ and $C_j$.\n",
    "\n",
    "### Loss Function and Optimization\n",
    "To learn $\\theta^{(i, j)}$, we minimize the binary cross-entropy loss for the data points belonging to classes $C_i$ and $C_j$, denoted $X^{(i, j)}$:\n",
    "\n",
    "$\\mathcal{L}^{(i, j)}(\\theta^{(i, j)}) = - \\sum_{k \\in X^{(i, j)}} \\left( y_k^{(i, j)} \\log P(y = 1 | x_k; \\theta^{(i, j)}) + (1 - y_k^{(i, j)}) \\log P(y = 0 | x_k; \\theta^{(i, j)}) \\right)$\n",
    "\n",
    "We use gradient-based optimization (e.g., gradient descent) to minimize this loss function for each pair.\n",
    "\n",
    "### Pseudocode\n",
    "input:  \n",
    "\n",
    "    training set $S = (x_1,y_1),...,(x_m,y_m)$  \n",
    "\n",
    "    algorithm for binary classification A   \n",
    "\n",
    "for each $i,j \\in \\mathcal{Y} s.t. i < j$  \n",
    "\n",
    "    initialize $S_{i,j}$ to be the empty sequence  \n",
    "\n",
    "    for t = 1,...,m  \n",
    "\n",
    "        If $y_t = i$ add $(x_t,1)$ to $S_{ij}$  \n",
    "\n",
    "        If $y_t = j$ add $(x_t,-1)$ to $S_{ij}$  \n",
    "\n",
    "    Let $h_{i,j} = A(S_{ij})$  \n",
    "\n",
    "output:  \n",
    "\n",
    "    the multiclass hypothesis defined by $h(x) \\in argmax_{i \\in \\mathcal{Y}}(\\sum_{j \\in \\mathcal{Y}} sign(j-i) h_{i,j}(x))$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
