{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb88370c",
   "metadata": {},
   "source": [
    "# **Multiclass Classification: One-vs-all & all-pairs algorithms**\n",
    "\n",
    "For our final project, we plan on implementing the one-vs-all and the all-pairs algorithms we covered in Lecture 5 using logistic regression for binary classification. We plan on comparing the results of each multiclass classification approach to evaluate their performance in terms of accuracy, efficiency, and suitability for our particular dataset.\n",
    "\n",
    "\n",
    "### **One-vs-all Algorithm**\n",
    "\n",
    "One-vs-all is an approach to multiclass classification that converts a multiclass problem into multiple binary classification problems. The process involves first creating a separate binary classifier for each class in the dataset. Each classifier treats the class as the \"positive\" class and all the other classes as the \"negative\" class. For a given data point, we run each of these binary classification algorithms and output the class that corresponds to the highest predicted probability. \n",
    "\n",
    "$\\text{Training data: } {\\bf X} \\text{ (features), } {\\bf y} \\text{ (labels) with } k \\text{ classes}$ <br />\n",
    "$\\text{Logistic regression model for binary classification}$ <br />\n",
    "\n",
    "1. $\\text{Initialize an empty list, } \\textit{models}, \\text{to store each class's logistic regression model}$ <br />\n",
    "2. $\\text{For each class } i \\text{ in range } 1 \\text{ to } k:$ <br />\n",
    "     $\\quad$ a. $\\text{Create a new binary label vector } y_i \\text{ where:}$ <br />\n",
    "     $\\quad \\quad$ - $y_i[j] = 1$ $\\text{ if } y[j] = i \\text{ (current class)}$ <br />\n",
    "     $\\quad \\quad$ - $y_i[j] = 0$ $\\text{ otherwise (all other classes)}$ <br />\n",
    "     $\\quad$ b. $\\text{Train a logistic regression model } model_i \\text{ using } {\\bf X} \\text{ and } y_i$ <br />\n",
    "     $\\quad$ c. $\\text{Store } model_i \\text{ in the list } \\textit{models}$ <br />\n",
    "\n",
    "3. $\\text{For a given input } {\\bf x}:$ <br />\n",
    "     $\\text{For each model } i \\text{ in range } 1 \\text{ to } k:$ <br />\n",
    "        $\\quad \\quad$ - $\\text{Use each model in } \\textit{models} \\text{ to predict the probability that } {\\bf x} \\text{ belongs to each class}$ <br />\n",
    "        $\\quad \\quad$ - $\\text{Store the probabilities in a list } \\textit{probabilities}$ <br />\n",
    "     $\\text{Select the class with the highest probability from } \\textit{probabilities} \\text{ as the predicted class}$ <br />\n",
    "\n",
    "4. $\\text{Return the predicted classes}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b83161-d6ea-4ae5-b36f-9457f6c20ec3",
   "metadata": {},
   "source": [
    "Logistic Regression uses the sigmoid function, which is defined as follows:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where z is $\\langle w,x \\rangle$\n",
    "This function takes in values of $X = \\mathbb{R}^d$ and outputs continuous values in [0,1] that correspond to probabilities that are used to classify the points as $Y$ = {1, -1}.\n",
    "\n",
    "The decision boundary based on this classifier is still $\\langle w,x \\rangle = 0$ and corresponds to a probability of 50%.\n",
    "\n",
    "Now moving on to the loss for logisitic regression, in the binary case, log loss is as follows:\n",
    "$$\n",
    "\\ell(h_{\\mathbf{w}}, (\\mathbf{x}, y)) = \\log(1 + \\exp(-y \\langle \\mathbf{w}, \\mathbf{x} \\rangle))\n",
    "$$\n",
    "\n",
    "This loss function penalizes the degree of wrongness in the case of misclassification.\n",
    "\n",
    "Log loss is also convex, which moves us onto the optimization of the loss function. The optimization is done according to empirical risk minimization, which aims to find the hypothesis within the hypothesis class that minimizes the expected loss over all available data. In other words, ERM selects the hypothesis that produces the lowest average loss on the entire dataset. Since log-loss is convex, it is known that there is at most one global minimum, which would be where the loss is the smallest. \n",
    "In order to find this minimima, the gradients of $L(w)$ are computed with respect to each weight $w_j$. This method called gradient descent is used to iteratively minimize the loss function by adjusting the model parameters in the direction that reduces loss. Specifically, the weights are updated iteratively as follows:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$\\alpha$ in this equation is the learning rate, which controls the size of the steps taken during gradient descent to update model parameters. It is important to select this parameter carefully because an overy large $\\alpha$ can cause the model to overshoot the optimal values, while an overly small $\\alpha$ can result in slow convergence or getting stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24efd6c-d83a-4e0a-9ba5-1e07b6b156af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
