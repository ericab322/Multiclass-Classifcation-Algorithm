{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Overview of Multiclass Classification with Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Logistic Regression is a technique for categorizing samples into one of three or more classes. While logistic regression is inherently designed for binary classification, it can be extended to handle multiclass problems using techniques such as the **One vs. All** and **All-Pairs** approaches. Both methods leverage binary logistic regression classifiers for making multiclass predictions, but they employ them in fundamentally different ways. The **One vs. All** treats each class separately against all others, while the **All-Pairs** approach trains a binary classifier for every pair of classes and combines their outputs. For binary logistic regression, the sigmoid function is used for representation, outputting probabilites and defining the decision boundary at 0.5. The log loss function measures the difference between predicted probabilities and actual labels, guiding optimization through stochastic gradient descent (SGD). Training continues until either a maximum of 1000 epochs is reached or the convergence threshold of $1 \\times 10^4$ is met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression Math \n",
    "Logistic Regression uses the sigmoid function, which is defined as follows:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where z is $\\langle w,x \\rangle$\n",
    "This function takes in values of $X = \\mathbb{R}^d$ and outputs continuous values in [0,1] that correspond to probabilities that are used to classify the points as $Y$ = {1, -1}.\n",
    "\n",
    "The decision boundary based on this classifier is still $\\langle w,x \\rangle = 0$ and corresponds to a probability of 50%.\n",
    "\n",
    "Now moving on to the loss for logisitic regression, in the binary case, log loss is as follows:\n",
    "$$\n",
    "\\ell(h_{\\mathbf{w}}, (\\mathbf{x}, y)) = \\log(1 + \\exp(-y \\langle \\mathbf{w}, \\mathbf{x} \\rangle))\n",
    "$$\n",
    "\n",
    "This loss function penalizes the degree of wrongness in the case of misclassification.\n",
    "\n",
    "Log loss is also convex, which moves us onto the optimization of the loss function. The optimization is done according to empirical risk minimization, which aims to find the hypothesis within the hypothesis class that minimizes the expected loss over all available data. In other words, ERM selects the hypothesis that produces the lowest average loss on the entire dataset. Since log-loss is convex, it is known that there is at most one global minimum, which would be where the loss is the smallest. \n",
    "In order to find this minimima, the gradients of $L(w)$ are computed with respect to each weight $w_j$. This method, called gradient descent, is used to iteratively minimize the loss function by adjusting the model parameters in the direction that reduces loss. Specifically, the weights are updated iteratively as follows:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$\\alpha$ in this equation is the learning rate, which controls the size of the steps taken during gradient descent to update model parameters. It is important to select this parameter carefully because an overy large $\\alpha$ can cause the model to overshoot the optimal values, while an overly small $\\alpha$ can result in slow convergence or getting stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression Pseudocode\n",
    "\n",
    "1. **Initialize Parameters**:\n",
    "   - Initialize weights `W` as a vector of small random values or zeros.\n",
    "   - Initialize bias `b` as a small random value or zero.\n",
    "\n",
    "2. **Sigmoid Function**:\n",
    "   ```python\n",
    "   def sigmoid(z):\n",
    "       return 1 / (1 + exp(-z))\n",
    "\n",
    "      z = W * X + b\n",
    "      y_hat = sigmoid(z)\n",
    "\n",
    "      # Compute loss\n",
    "      loss = - (1 / m) * sum(y * log(y_hat) + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "      # Compute gradients\n",
    "      dW = (1 / m) * X.T * (y_hat - y)  # Gradient of loss with respect to W\n",
    "      db = (1 / m) * sum(y_hat - y)     # Gradient of loss with respect to b\n",
    "\n",
    "      # Update parameters\n",
    "      W = W - learning_rate * dW\n",
    "      b = b - learning_rate * db\n",
    "\n",
    "      def predict(X_new):\n",
    "         z_new = W * X_new + b\n",
    "         y_new_hat = sigmoid(z_new)\n",
    "\n",
    "         if y_new_hat >= 0.5:\n",
    "            return 1\n",
    "         else:\n",
    "            return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All-Pairs\n",
    "\n",
    "1. **Training**\n",
    "\n",
    "   In the all-pairs approach for multi-class classification, multiple binary logistic regression models are trained for each pair of classes. Here’s the math involved:\n",
    "\n",
    "   The total number of unique pairs of classes for $K$ classes is:\n",
    "\n",
    "   $$\\text{Number of pairs} = \\binom{K}{2} = \\frac{K(K - 1)}{2}$$\n",
    "\n",
    "   For each pair of classes $(C_i, C_j)$, we train a binary logistic regression model to distinguish between data points in $C_i$ and $C_j$.\n",
    "\n",
    "2. **Probability Estimation**\n",
    "   The probability that a data point $x$ belongs to class $C_i$ rather than $C_j$ is given by:\n",
    "\n",
    "   $$P(y = 1 | x; \\theta^{(i, j)}) = \\frac{1}{1 + e^{-\\theta^{(i, j) \\top} x}}$$\n",
    "\n",
    "   where $ \\theta^{(i, j)}$ is the parameter vector specific to the classifier for classes $C_i$ and $C_j$.\n",
    "\n",
    "\n",
    "3. **Pseudocode**\n",
    "\n",
    "   #### Train method:\n",
    "   **Input**:  \n",
    "      - Training data  $X$ (features) and $Y$ (labels)  \n",
    "      - Binary logistic regression model\n",
    "\n",
    "   **Steps**:\n",
    "      1. **Validate input data** to ensure that $X $ and $Y$ are correctly formatted and consistent.\n",
    "      2. **Create all possible class pairs** $(C_i, C_j)$ where $C_i < C_j$. This results in the set of class pairs for multi-class classification.\n",
    "      3. **For each pair of classes** $(C_i, C_j)$:\n",
    "         - Create a **mask** where $Y$ is either $C_i$ or $C_j$. .\n",
    "         - **Filter** the training data $X$ and labels \\( Y \\) using the mask to get the sub-dataset $S_X$ and corresponding labels $S_Y$.\n",
    "         - **Convert** $S_Y$ to binary values $[1, 0]$, where data points from $C_i$ are labeled 1 and those from $C_j$ are labeled 0.\n",
    "      4. **Initialize and train** a binary logistic regression classifier using $S_X$ and the binary $S_Y$ labels.\n",
    "      5. **Store** the trained classifier for later use in the prediction phase.\n",
    "\n",
    "   ### Predict method:\n",
    "   **Input**:  \n",
    "      - Training data $X$ (features) and $Y$ (labels)  \n",
    "      - Binary logistic regression model\n",
    "\n",
    "   **Steps**:\n",
    "      1. **Validate input data** to ensure that $X$ and $Y$ are correctly formatted and consistent. \n",
    "      2. **Initialize a vote array** with zeros to store votes for each class for each sample in $X$. \n",
    "      3. **For each pair of classes** $(C_i, C_j)$ and their respective classifiers: \n",
    "         - Use the classifier to **predict binary labels** (either 1 or 0).\n",
    "         - If the predicted label is 1, **add a vote to $C_i$**. \n",
    "         - If the predicted label is 0, **add a vote to $C_j$**.\n",
    "      4. **For each sample in $X$**, assign the **class label** corresponding to the class with the highest vote count. \n",
    "      5. **Return** the predicted class label for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-all Algorithm\n",
    "\n",
    "\n",
    "One-vs-all is an approach to multiclass classification that converts a multiclass problem into multiple binary classification problems. The process involves first creating a separate binary classifier for each class in the dataset. Each classifier treats the class as the \"positive\" class and all the other classes as the \"negative\" class. For a given data point, we run each of these binary classification algorithms and output the class that corresponds to the highest predicted probability. \n",
    "\n",
    "\n",
    "1. **Training**\n",
    "\n",
    "  In the one-vs-all approach for multi-class classification, multiple binary logistic regression models are trained, one for each class. Here’s the math involved:\n",
    "\n",
    "   For $K$ classes, we train $K$ binary classifiers. Each classifier $i$ is trained to distinguish between the data points in class $C_i$ and all other classes.\n",
    "\n",
    "   The binary labels for the classifier corresponding to class $C_i$ are:\n",
    "    $$y = \n",
    "            \\begin{cases}\n",
    "                1 & \\text{if} \\; x \\; \\text{if the data point belongs to class $C_{i}$} \\; \\\\\n",
    "                0 & \\text{if} \\; x \\; \\text{otherwise} \\; \n",
    "            \\end{cases}$$\n",
    "   \n",
    "2. **Probability Estimation**\n",
    "\n",
    "The probability that a data point $x$ belongs to class $C_i$ is given by:\n",
    "\n",
    "   $$P(y = 1 | x; \\theta^{(i, j)}) = \\frac{1}{1 + e^{-\\theta^{(i, j) \\top} x}}$$\n",
    "\n",
    "   where $\\theta^{(i)}$ is the parameter vector specific to the classifier for class $C_i$.\n",
    "\n",
    "\n",
    "3. **Pseudocode**\n",
    "\n",
    "   #### Train method:\n",
    "   **Input**:  \n",
    "      - Training data  $X$ (features) and $Y$ (labels)  \n",
    "      - Binary logistic regression model\n",
    "\n",
    "   **Steps**:\n",
    "\n",
    "      1. $\\text{Initialize an empty list, } \\textit{models}, \\text{to store each class's logistic regression model}$ <br />\n",
    "      2. $\\text{For each class } i \\text{ in range } 1 \\text{ to } k:$ <br />\n",
    "         $\\quad$ a. $\\text{Create a new binary label vector } y_i \\text{ where:}$ <br />\n",
    "         $\\quad \\quad$ - $y_i[j] = 1$ $\\text{ if } y[j] = i \\text{ (current class)}$ <br />\n",
    "         $\\quad \\quad$ - $y_i[j] = 0$ $\\text{ otherwise (all other classes)}$ <br />\n",
    "         $\\quad$ b. $\\text{Initialize and train a logistic regression model } model_i \\text{ using } {\\bf X} \\text{ and } y_i$ <br />\n",
    "         $\\quad$ c. $\\text{Store } model_i \\text{ in the list } \\textit{models}$ <br />\n",
    "\n",
    "     **Output**:\n",
    "     A list of $K$ trained binary classifiers.\n",
    "\n",
    "   #### Predict method:\n",
    "   **Input**:  \n",
    "      - Test data $X$ (features) and $Y$ (labels)  \n",
    "      - Trained binary classifiers\n",
    "\n",
    "   **Steps**:\n",
    "   1. **Validate input data** to ensure that $X$ and $Y$ are correctly formatted and consistent. \n",
    "   2. **Initialize a probability** array with shape $(N, K)$, where $N$ is the number of test samples and $K$ is the number of classes.\n",
    "   3. **For each class** $C_i$ and its respective classifier:\n",
    "         - Use the classifier to **predict probabilities** for all samples in $X$\n",
    "         - Store the probabilities in the $i$-th column of the probability array.\n",
    "   4. **For each sample in $X$**, assign the **class label** corresponding to the class with the highest highest probability. (np.argmax)\n",
    "   5. **Return** the predicted class label for each sample.\n",
    "      \n",
    "      **Output**:\n",
    "      An array of predicted class labels for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "\n",
    "### One-vs-All\n",
    "\n",
    "**Advantages**:\n",
    "1. **Simplicity**: The One-vs-All method is conceptually straightforward and easily-implemented. It decomposes the multiclass problem into multiple independent binary classification tasks, which can be handled by standard binary logistic regression classifiers.\n",
    "2. **Efficiency**: For a dataset with $N$ classes, OvR requires training only $N$ classifiers, making it computationally efficient for smaller class sizes.\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **Class Imbalance**: If the classes are imbalanced, some classifiers could be biased toward the dominant class, which could lead to suboptimal performance.\n",
    "2. **Overlapping Classes**: This method assumes that each class is independent of the others. When classes have significant overlap, this can cause poor performance as the decision boundaries learned by each classifier may not capture the relationships between classes.\n",
    "3. **Suboptimal Decision Boundaries**: Since the classifiers are trained independently, they may not effectively handle interactions between classes, potentially leading to decision boundaries that are not optimal for multiclass tasks.\n",
    "\n",
    "### All-Pairs\n",
    "\n",
    "**Advantages**:\n",
    "1. **Higher Accuracy**: The All-Pairs method often performs better than One-vs-All, as it explicitly models pairwise relationships between classes. This method captures more complex decision boundaries that can lead to improved prediction accuracy.\n",
    "2. **Captures Class Interactions**: Since All-Pairs trains on class pairs, All-Pairs can capture inter-class relationships more effectively, which is useful when classes have overlapping features.\n",
    "3. **Improved Generalization**: Because the method takes into account pairwise comparisons, it can generalize better in situations where the decision boundaries are not easily separable by individual classifiers.\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **Computational Complexity**: All-Pairs requires training $\\binom{N}{2}$ classifiers, which grows with the number of classes. This can be impractical for problems with a large number of classes.\n",
    "2. **Prediction Complexity**: During prediction, the outputs of many classifiers must be combined, which increases the complexity of the model and can lead to slower prediction times compared to One Vs. All.\n",
    "3. **Scalability Issues**: While All-Pairs can offer better performance, the computational efficiency decreases as the number of classes increases, making it less scalable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, n_features, batch_size, conv_threshold = 1e-4, max_epochs = 100, random_state = None):\n",
    "        \"\"\"Initialize the binary logistic regression model.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        if not isinstance(n_features, int) or n_features <= 0:\n",
    "            raise ValueError(\"`n_features` must be a positive integer.\")\n",
    "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "            raise ValueError(\"`batch_size` must be a positive integer.\")\n",
    "        if not isinstance(conv_threshold, (int, float)) or conv_threshold <= 0:\n",
    "            raise ValueError(\"`conv_threshold` must be a positive number.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "            raise ValueError(\"`max_epochs` must be a positive number.\")\n",
    "        if random_state is not None and not isinstance(random_state, int):\n",
    "            raise ValueError(\"`random_state` must be an integer or None.\")\n",
    "            \n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros(n_features + 1)  # extra element for bias\n",
    "        self.alpha = 0.01\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.max_epochs = max_epochs\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Perform sigmoid operation\n",
    "        @params:\n",
    "            z: the input to which sigmoid will be applied\n",
    "        @return:\n",
    "            an array with sigmoid applied elementwise.\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''self.epochs\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n",
    "            raise TypeError(\"`X` and `Y` must be Numpy arrays.\")\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"`X` and `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"mismatch in # of samples between `X` and `Y`.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "        if not np.array_equal(Y, Y.astype(int)) or not np.all((Y == 0) | (Y == 1)):\n",
    "            raise ValueError(\"`Y` must contain binary labels (0 or 1).\")\n",
    "\n",
    "        # intializing values\n",
    "        epochs = 0\n",
    "        n_examples = X.shape[0]\n",
    "        X_bias = np.hstack([X, np.ones((X.shape[0], 1))])  # Append bias term\n",
    "\n",
    "        for i in range(self.max_epochs):\n",
    "            # update # of epochs\n",
    "            epochs +=1\n",
    "            # acquire indices for shuffling of X and Y\n",
    "            indices = np.arange(n_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_bias = X_bias[indices]\n",
    "            Y = Y[indices]\n",
    "            # calc last epoch loss\n",
    "            last_epoch_loss = self.loss(X, Y)\n",
    "            # for the # of batches\n",
    "            for i in range(0, n_examples, self.batch_size):\n",
    "                X_batch = X_bias[i:i + self.batch_size]\n",
    "                Y_batch = Y[i:i + self.batch_size]\n",
    "                # reinitialize gradient to be 0s\n",
    "                grad = np.zeros(self.weights.shape)\n",
    "                # for each pair in the batch\n",
    "                for x, y in zip(X_batch, Y_batch):\n",
    "                    prediction = self.sigmoid(self.weights @ x) #np.dot(self.weights, x))\n",
    "                    # gradient calculation\n",
    "                    error = prediction - y\n",
    "                    grad += error * x\n",
    "                # update weights\n",
    "                self.weights -= ((self.alpha * grad)/ self.batch_size)\n",
    "            epoch_loss = self.loss(X, Y)\n",
    "            if abs(epoch_loss - last_epoch_loss) < self.conv_threshold:\n",
    "                break\n",
    "        return epochs\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n",
    "            raise TypeError(\"`X` and `Y` must be Numpy arrays.\")\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"`X` and `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"mismatch in # of samples between `X` and `Y`.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "        if not np.array_equal(Y, Y.astype(int)) or not np.all((Y == 0) | (Y == 1)):\n",
    "            raise ValueError(\"`Y` must contain binary labels (0 or 1).\")\n",
    "        \n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])  # Append bias term\n",
    "        n_examples = X.shape[0]\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(n_examples):\n",
    "            # linear output (dot product)\n",
    "            linear_output = X[i] @ self.weights.T  #np.dot(self.weights, X[i].T)\n",
    "            # calc logistic loss for each sample\n",
    "            y = 1 if Y[i] == 1 else -1\n",
    "            logistic_loss = np.log(1 + np.exp(-y * linear_output))\n",
    "            total_loss += logistic_loss\n",
    "    \n",
    "        return total_loss / n_examples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(\"`X` must be a Numpy array.\")\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"`X` cannot be empty.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "            \n",
    "        # multiply X by weights of model\n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])  # Append bias term\n",
    "        predictions = self.sigmoid(X @ self.weights.T)\n",
    "        return np.where(predictions >= 0.5, 1, 0)\n",
    "        \n",
    "    def predict_probs(self, X):\n",
    "        '''\n",
    "        Compute prediction probabilities based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            an array with sigmoid applied elementwise.\n",
    "        '''\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(\"`X` must be a Numpy array.\")\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"`X` cannot be empty.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "            \n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])  # Append bias term\n",
    "        predictions = self.sigmoid(X @ self.weights.T)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n",
    "            raise TypeError(\"`X` and `Y` must be Numpy arrays.\")\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"`X` and `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"mismatch in # of samples between `X` and `Y`.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All-Pairs Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class AllPairsLogisticRegression:\n",
    "    def __init__(self, n_classes, binary_classifier_class, n_features, batch_size, max_epochs = 100, conv_threshold = 1e-6, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the all-pairs logistic regression model approach.\n",
    "        @param n_classes: Number of classes in the dataset, an integer.\n",
    "        @param binary_classifier_class: Class for binary logistic regression, a class object.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training the binary classifiers, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(n_classes, int) or n_classes <= 1:\n",
    "            raise ValueError(\"`n_classes` must be an integer greater than 1.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "            raise ValueError(\"`epochs` must be an integer greater than 0.\")\n",
    "        if not callable(binary_classifier_class):\n",
    "            raise TypeError(\"`binary_classifier_class` must be a callable class.\")\n",
    "        if not isinstance(n_features, int) or n_features <= 0:\n",
    "            raise ValueError(\"`n_features` must be a positive integer.\")\n",
    "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "            raise ValueError(\"`batch_size` must be a positive integer.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "                raise ValueError(\"`max_epochs` must be a positive number.\")\n",
    "        if not isinstance(conv_threshold, (int, float)) or conv_threshold <= 0:\n",
    "            raise ValueError(\"`conv_threshold` must be a positive number.\")\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.classifiers = {}\n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.binary_classifier_class = binary_classifier_class\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the all-pairs logistic regression model by training binary classifiers\n",
    "        for each pair of classes in the dataset.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: Labels of the dataset, a numpy array of shape (n_samples).\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"Input data `X` and labels `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Mismatch in number of samples between `X` and `Y`.\")\n",
    "        if np.any((Y < 0) | (Y >= self.n_classes)):\n",
    "            raise ValueError(f\"Labels in `Y` must be in the range [0, {self.n_classes - 1}].\")\n",
    "        unique_classes = np.arange(self.n_classes)\n",
    "        pairs = [(class_i, class_j) for class_i in unique_classes for class_j in unique_classes if class_i < class_j]\n",
    "\n",
    "        for class_i, class_j in pairs:\n",
    "            mask = (Y == class_i) | (Y == class_j)\n",
    "            SX = X[mask]\n",
    "            SY = np.where(Y[mask] == class_i, 1, 0)\n",
    "            classifier = self.binary_classifier_class(\n",
    "                n_features=self.n_features,\n",
    "                batch_size=self.batch_size,\n",
    "                max_epochs=self.max_epochs, random_state=self.random_state,\n",
    "                conv_threshold = self.conv_threshold\n",
    "            )\n",
    "            classifier.train(SX, SY)\n",
    "            self.classifiers[(class_i, class_j)] = classifier\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data using the trained classifiers.\n",
    "        @param X: Input features to classify, a numpy array of shape (n_samples, n_features).\n",
    "        @return: Predicted class labels, a numpy array of shape (n_samples).\n",
    "        \"\"\"\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Input data `X` cannot be empty.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        votes = np.zeros((n_samples, self.n_classes), dtype=int)\n",
    "        for (class_i, class_j), classifier in self.classifiers.items():\n",
    "            predictions = classifier.predict(X)\n",
    "            votes[:, class_i] += (predictions == 1)\n",
    "            votes[:, class_j] += (predictions == 0)\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data and labels by finding ratio of correct predictions to total samples.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: True labels of the dataset, a numpy array of shape (n_samples).\n",
    "        @return: Accuracy of the model as a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"Input data `X` and labels `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Mismatch in number of samples between `X` and `Y`.\")\n",
    "        \n",
    "        predictions = self.predict(X)\n",
    "        correct_predictions = np.sum(predictions == Y)\n",
    "        return correct_predictions / len(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-all Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OneVsAllLogisticRegression:\n",
    "    def __init__(self, n_classes, binary_classifier_class, n_features, batch_size, max_epochs = 100, conv_threshold = 1e-6, random_state = None):\n",
    "        \"\"\"\n",
    "        Initialize the One-vs-All logistic regression model.\n",
    "        @param n_classes: Number of classes in the dataset, an integer.\n",
    "        @param binary_classifier_class: Class for binary logistic regression, a class object.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training the binary classifiers, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(n_classes, int) or n_classes <= 1:\n",
    "            raise ValueError(\"`n_classes` must be an integer greater than 1.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "            raise ValueError(\"`epochs` must be an integer greater than 0.\")\n",
    "        if not callable(binary_classifier_class):\n",
    "            raise TypeError(\"`binary_classifier_class` must be a callable class.\")\n",
    "        if not isinstance(n_features, int) or n_features <= 0:\n",
    "            raise ValueError(\"`n_features` must be a positive integer.\")\n",
    "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "            raise ValueError(\"`batch_size` must be a positive integer.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "                raise ValueError(\"`max_epochs` must be a positive number.\")\n",
    "        if not isinstance(conv_threshold, (int, float)) or conv_threshold <= 0:\n",
    "            raise ValueError(\"`conv_threshold` must be a positive number.\")\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.classifiers = {}  \n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs \n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.binary_classifier_class = binary_classifier_class\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the One-vs-All logistic regression model by training one binary classifier\n",
    "        for each class in the dataset.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: Labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"Input data `X` and labels `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Mismatch in number of samples between `X` and `Y`.\")\n",
    "        if np.any((Y < 0) | (Y >= self.n_classes)):\n",
    "            raise ValueError(f\"Labels in `Y` must be in the range [0, {self.n_classes - 1}].\")\n",
    "        \n",
    "        for class_i in range(self.n_classes):\n",
    "            # Create binary labels: 1 for the current class, 0 for others\n",
    "            binary_labels = np.where(Y == class_i, 1, 0)\n",
    "            classifier = self.binary_classifier_class(\n",
    "                n_features=self.n_features,\n",
    "                batch_size=self.batch_size,\n",
    "                max_epochs=self.max_epochs, random_state=self.random_state,\n",
    "                conv_threshold = self.conv_threshold\n",
    "            )\n",
    "            classifier.train(X, binary_labels)\n",
    "            self.classifiers[class_i] = classifier\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data using the trained classifiers.\n",
    "        @param X: Input features to classify, a numpy array of shape (n_samples, n_features).\n",
    "        @return: Predicted class labels, a numpy array of shape (n_samples,).\n",
    "        \"\"\"\n",
    "\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Input data `X` cannot be empty.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        scores = np.zeros((n_samples, self.n_classes))\n",
    "\n",
    "        for class_i, classifier in self.classifiers.items():\n",
    "            # Get probabilities for the current class\n",
    "            scores[:, class_i] = classifier.predict_probs(X)\n",
    "\n",
    "        # Select the class with the highest probability/score for each sample\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data and labels.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: True labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: Accuracy of the model as a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"Input data `X` and labels `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Mismatch in number of samples between `X` and `Y`.\")\n",
    "        \n",
    "        preds = self.predict(X)\n",
    "        acc = np.mean(preds == Y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_gradient_calculation (__main__.TestBinaryLogisticRegression.test_gradient_calculation)\n",
      "Test that the gradient is computed correctly. ... ok\n",
      "test_imbalanced_data (__main__.TestBinaryLogisticRegression.test_imbalanced_data)\n",
      "Test behavior on an imbalanced dataset. ... ok\n",
      "test_invalid_accuracy_inputs (__main__.TestBinaryLogisticRegression.test_invalid_accuracy_inputs)\n",
      "Test invalid inputs for accuracy calculation. ... ok\n",
      "test_invalid_initialization (__main__.TestBinaryLogisticRegression.test_invalid_initialization)\n",
      "Test invalid parameters during initialization. ... ok\n",
      "test_invalid_loss_inputs (__main__.TestBinaryLogisticRegression.test_invalid_loss_inputs)\n",
      "Test invalid inputs for the loss function. ... ok\n",
      "test_invalid_predict_inputs (__main__.TestBinaryLogisticRegression.test_invalid_predict_inputs)\n",
      "Test invalid prediction inputs for X. ... ok\n",
      "test_invalid_train_inputs (__main__.TestBinaryLogisticRegression.test_invalid_train_inputs)\n",
      "Test invalid training inputs for X and Y. ... ok\n",
      "test_loss_initialization (__main__.TestBinaryLogisticRegression.test_loss_initialization)\n",
      "Test that the initial loss is computed correctly. ... ok\n",
      "test_new_unseen_data (__main__.TestBinaryLogisticRegression.test_new_unseen_data)\n",
      "Test the model on unseen data after training. ... ok\n",
      "test_non_separable_data (__main__.TestBinaryLogisticRegression.test_non_separable_data)\n",
      "Test behavior with non-linearly separable data. ... ok\n",
      "test_sigmoid (__main__.TestBinaryLogisticRegression.test_sigmoid)\n",
      "Test that sigmoid outputs correct calculations. ... ok\n",
      "test_training_and_predictions (__main__.TestBinaryLogisticRegression.test_training_and_predictions)\n",
      "Test that the model learns correctly on training data. ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.239s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class TestBinaryLogisticRegression(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Initialize common test parameters.\"\"\"\n",
    "        self.n_features = 1\n",
    "        self.batch_size = 1\n",
    "        self.conv_threshold = 1e-6\n",
    "        self.max_epochs = 1000\n",
    "        self.model = BinaryLogisticRegression(\n",
    "            n_features=self.n_features,\n",
    "            batch_size=self.batch_size,\n",
    "            conv_threshold=self.conv_threshold,\n",
    "            max_epochs = self.max_epochs\n",
    "        )\n",
    "\n",
    "    def test_loss_initialization(self):\n",
    "        \"\"\"Test that the initial loss is computed correctly.\"\"\"\n",
    "        np.random.seed(0)\n",
    "        x = np.array([[1], [2], [3], [4], [5], [1.2]])\n",
    "        y = np.array([0, 0, 1, 1, 1, 0])\n",
    "        initial_loss = self.model.loss(x, y)\n",
    "        self.assertAlmostEqual(initial_loss, 0.693, places=3)\n",
    "\n",
    "    def test_sigmoid(self):\n",
    "        \"\"\"Test that sigmoid outputs correct calculations.\"\"\"\n",
    "        # small positive input\n",
    "        z = 1\n",
    "        expected_output = 1 / (1 + np.exp(-z))\n",
    "        output = self.model.sigmoid(z)\n",
    "        self.assertAlmostEqual(output, expected_output, places=6)\n",
    "\n",
    "        # small negative input\n",
    "        z = -1\n",
    "        expected_output = 1 / (1 + np.exp(-z))\n",
    "        output = self.model.sigmoid(z)\n",
    "        self.assertAlmostEqual(output, expected_output, places=6)\n",
    "\n",
    "        # large positive input (ensure stability)\n",
    "        z = 100\n",
    "        expected_output = 1.0  # Sigmoid saturates to 1\n",
    "        output = self.model.sigmoid(z)\n",
    "        self.assertAlmostEqual(output, expected_output, places=6)\n",
    "\n",
    "        # large negative input (ensure stability)\n",
    "        z = -100\n",
    "        expected_output = 0.0  # Sigmoid saturates to 0\n",
    "        output = self.model.sigmoid(z)\n",
    "        self.assertAlmostEqual(output, expected_output, places=6)\n",
    "\n",
    "        # zero input\n",
    "        z = 0\n",
    "        expected_output = 0.5  # Sigmoid(0) = 0.5\n",
    "        output = self.model.sigmoid(z)\n",
    "        self.assertAlmostEqual(output, expected_output, places=6)\n",
    "\n",
    "        # vector input\n",
    "        z = np.array([-1, 0, 1])\n",
    "        expected_output = 1 / (1 + np.exp(-z))\n",
    "        output = self.model.sigmoid(z)\n",
    "        np.testing.assert_array_almost_equal(output, expected_output, decimal=6)\n",
    "\n",
    "    def test_training_and_predictions(self):\n",
    "        \"\"\"Test that the model learns correctly on training data.\"\"\"\n",
    "        x = np.array([[1], [2], [3], [4], [5], [1.2]])\n",
    "        y = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "        self.model.train(x, y)\n",
    "        predictions = self.model.predict(x)\n",
    "        expected_predictions = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "        np.testing.assert_array_equal(predictions, expected_predictions)\n",
    "        accuracy = self.model.accuracy(x, expected_predictions)\n",
    "        self.assertAlmostEqual(accuracy, 1.0, places=2)\n",
    "\n",
    "    def test_new_unseen_data(self):\n",
    "        \"\"\"Test the model on unseen data after training.\"\"\"\n",
    "        x_train = np.array([[1], [2], [3], [4], [5], [1.2]])\n",
    "        y_train = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "        x_test = np.array([[1.5], [3.5]])\n",
    "        y_test = np.array([0, 1])\n",
    "\n",
    "        self.model.train(x_train, y_train)\n",
    "        predictions = self.model.predict(x_test)\n",
    "        np.testing.assert_array_equal(predictions, y_test)\n",
    "        accuracy = self.model.accuracy(x_test, y_test)\n",
    "        self.assertAlmostEqual(accuracy, 1.0, places=2)\n",
    "\n",
    "    def test_gradient_calculation(self):\n",
    "        \"\"\"Test that the gradient is computed correctly.\"\"\"\n",
    "        weights = np.array([0.0, 0.0])  # init weights\n",
    "        x_sample = np.array([1])  # single feature\n",
    "        y_sample = 0\n",
    "\n",
    "        z = weights[0] * x_sample[0] + weights[1]  # using weights and bias term\n",
    "        prediction = 1 / (1 + np.exp(-z))\n",
    "        gradient = (prediction - y_sample) * np.array([x_sample[0], 1])  # gradient for weight + bias\n",
    "        expected_gradient = np.array([0.5, 0.5])  # manually computed gradient with zero weights\n",
    "\n",
    "        np.testing.assert_allclose(gradient, expected_gradient, atol=0.01)\n",
    "\n",
    "    def test_imbalanced_data(self):\n",
    "        \"\"\"Test behavior on an imbalanced dataset.\"\"\"\n",
    "        x = np.array([[1], [2], [3], [4], [5]])\n",
    "        # imbalanced labels\n",
    "        y = np.array([0, 0, 0, 0, 1])  \n",
    "        epochs = self.model.train(x, y)\n",
    "        predictions = self.model.predict(x)\n",
    "        majority_class = 0\n",
    "        self.assertGreaterEqual(np.sum(predictions == majority_class), 3)\n",
    "\n",
    "    def test_non_separable_data(self):\n",
    "        \"\"\"Test behavior with non-linearly separable data.\"\"\"\n",
    "        x = np.array([[1], [2], [3], [4]])\n",
    "        # Non-linearly separable\n",
    "        y = np.array([0, 1, 0, 1]) \n",
    "        self.model.train(x, y)\n",
    "        predictions = self.model.predict(x)\n",
    "        accuracy = self.model.accuracy(x, y)\n",
    "        # should be better than random guessing\n",
    "        self.assertGreaterEqual(accuracy, 0.5)  \n",
    "        self.assertLessEqual(accuracy, 1.0)\n",
    "\n",
    "     # FOR TESTING EDGE CASES INVOLVING FORMATTING OF DATA\n",
    "    def test_invalid_initialization(self):\n",
    "        \"\"\"Test invalid parameters during initialization.\"\"\"\n",
    "        # n_features\n",
    "        with self.assertRaises(ValueError):\n",
    "            BinaryLogisticRegression(n_features=-1, batch_size=1, conv_threshold=1e-3)\n",
    "        with self.assertRaises(ValueError):\n",
    "            BinaryLogisticRegression(n_features=0, batch_size=1, conv_threshold=1e-3)\n",
    "        # batch_size\n",
    "        with self.assertRaises(ValueError):\n",
    "            BinaryLogisticRegression(n_features=2, batch_size=0, conv_threshold=1e-3)\n",
    "        with self.assertRaises(ValueError):\n",
    "            BinaryLogisticRegression(n_features=2, batch_size=-5, conv_threshold=1e-3)\n",
    "        # conv_threshold\n",
    "        with self.assertRaises(ValueError):\n",
    "            BinaryLogisticRegression(n_features=2, batch_size=1, conv_threshold=-0.1)\n",
    "        with self.assertRaises(ValueError):\n",
    "            BinaryLogisticRegression(n_features=2, batch_size=1, conv_threshold=0)\n",
    "    \n",
    "    def test_invalid_train_inputs(self):\n",
    "        \"\"\"Test invalid training inputs for X and Y.\"\"\"\n",
    "        model = BinaryLogisticRegression(n_features=2, batch_size=1, conv_threshold=1e-3)\n",
    "        valid_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "        valid_Y = np.array([0, 1, 1])\n",
    "    \n",
    "        # non-numpy inputs\n",
    "        with self.assertRaises(TypeError):\n",
    "            model.train([[1, 2], [3, 4]], valid_Y)\n",
    "        with self.assertRaises(TypeError):\n",
    "            model.train(valid_X, [0, 1, 1])\n",
    "    \n",
    "        # empty inputs\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.train(np.array([]), valid_Y)\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.train(valid_X, np.array([]))\n",
    "    \n",
    "        # mismatched samples\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.train(valid_X, np.array([0, 1]))\n",
    "    \n",
    "        # invalid labels\n",
    "        with self.assertRaises(ValueError):\n",
    "            # 2 label is invalid\n",
    "            model.train(valid_X, np.array([0, 1, 2]))  \n",
    "    \n",
    "    def test_invalid_predict_inputs(self):\n",
    "        \"\"\"Test invalid prediction inputs for X.\"\"\"\n",
    "        model = BinaryLogisticRegression(n_features=2, batch_size=1, conv_threshold=1e-3)\n",
    "    \n",
    "        # non-numpy inputs\n",
    "        with self.assertRaises(TypeError):\n",
    "            model.predict([[1, 2], [3, 4]])\n",
    "    \n",
    "        # empty inputs\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.predict(np.array([]))\n",
    "    \n",
    "        # invlid feature dimensions\n",
    "        with self.assertRaises(ValueError):\n",
    "            # 3 features instead of 2\n",
    "            model.predict(np.array([[1, 2, 3]]))  \n",
    "    \n",
    "    def test_invalid_loss_inputs(self):\n",
    "        \"\"\"Test invalid inputs for the loss function.\"\"\"\n",
    "        model = BinaryLogisticRegression(n_features=2, batch_size=1, conv_threshold=1e-3)\n",
    "        valid_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "        valid_Y = np.array([0, 1, 1])\n",
    "    \n",
    "        # invalid feature dimensions\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.loss(np.array([[1, 2, 3]]), valid_Y)\n",
    "    \n",
    "        # invalid labels\n",
    "        with self.assertRaises(ValueError):\n",
    "            # 2 label is invalid\n",
    "            model.loss(valid_X, np.array([0, 1, 2])) \n",
    "    \n",
    "    def test_invalid_accuracy_inputs(self):\n",
    "        \"\"\"Test invalid inputs for accuracy calculation.\"\"\"\n",
    "        model = BinaryLogisticRegression(n_features=2, batch_size=1, conv_threshold=1e-3)\n",
    "        valid_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "        valid_Y = np.array([0, 1, 1])\n",
    "    \n",
    "        # mismatched samples\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.accuracy(valid_X, np.array([0, 1]))\n",
    "    \n",
    "        # empty inputs\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.accuracy(np.array([]), valid_Y)\n",
    "        with self.assertRaises(ValueError):\n",
    "            model.accuracy(valid_X, np.array([]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use `unittest.main()` with arguments to avoid conflicts with Jupyter Notebook.\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All-Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_accuracy (__main__.TestAllPairsLogisticRegression.test_accuracy)\n",
      "Test the accuracy calculation on training data. ... ok\n",
      "test_accuracy_calculation (__main__.TestAllPairsLogisticRegression.test_accuracy_calculation)\n",
      "Test that `accuracy` computes the correct value. ... ok\n",
      "test_all_classifiers_trained (__main__.TestAllPairsLogisticRegression.test_all_classifiers_trained)\n",
      "Test that all required binary classifiers are trained. ... ok\n",
      "test_invalid_batch_size (__main__.TestAllPairsLogisticRegression.test_invalid_batch_size)\n",
      "Test that an invalid `batch_size` parameter raises an error. ... ok\n",
      "test_invalid_binary_classifier_class (__main__.TestAllPairsLogisticRegression.test_invalid_binary_classifier_class)\n",
      "Test that an invalid `binary_classifier_class` raises an error. ... ok\n",
      "test_invalid_epochs (__main__.TestAllPairsLogisticRegression.test_invalid_epochs)\n",
      "Test that an invalid `max_epochs` parameter raises an error. ... ok\n",
      "test_invalid_n_classes (__main__.TestAllPairsLogisticRegression.test_invalid_n_classes)\n",
      "Test that an invalid `n_classes` parameter raises an error. ... ok\n",
      "test_invalid_n_features (__main__.TestAllPairsLogisticRegression.test_invalid_n_features)\n",
      "Test that an invalid `n_features` parameter raises an error. ... ok\n",
      "test_non_separable_data (__main__.TestAllPairsLogisticRegression.test_non_separable_data)\n",
      "Test the model on non-linearly separable data. ... ok\n",
      "test_predict_correct_classes (__main__.TestAllPairsLogisticRegression.test_predict_correct_classes)\n",
      "Test that `predict` returns the correct class labels. ... ok\n",
      "test_predict_invalid_dimensions (__main__.TestAllPairsLogisticRegression.test_predict_invalid_dimensions)\n",
      "Test that `predict` with invalid dimensions raises an error. ... ok\n",
      "test_predict_on_unseen_data (__main__.TestAllPairsLogisticRegression.test_predict_on_unseen_data)\n",
      "Test predictions on unseen testing data. ... ok\n",
      "test_train_creates_correct_classifiers_all_pairs (__main__.TestAllPairsLogisticRegression.test_train_creates_correct_classifiers_all_pairs)\n",
      "Test that `train` creates one classifier for each pair of classes and trains it correctly. ... ok\n",
      "test_train_dimension_mismatch (__main__.TestAllPairsLogisticRegression.test_train_dimension_mismatch)\n",
      "Test that training with mismatched dimensions raises an error. ... ok\n",
      "test_train_empty_data (__main__.TestAllPairsLogisticRegression.test_train_empty_data)\n",
      "Test that training with empty data raises an error. ... ok\n",
      "test_train_function (__main__.TestAllPairsLogisticRegression.test_train_function)\n",
      "Test the train function. ... ok\n",
      "test_gradient_calculation (__main__.TestBinaryLogisticRegression.test_gradient_calculation)\n",
      "Test that the gradient is computed correctly. ... ok\n",
      "test_imbalanced_data (__main__.TestBinaryLogisticRegression.test_imbalanced_data)\n",
      "Test behavior on an imbalanced dataset. ... ok\n",
      "test_invalid_accuracy_inputs (__main__.TestBinaryLogisticRegression.test_invalid_accuracy_inputs)\n",
      "Test invalid inputs for accuracy calculation. ... ok\n",
      "test_invalid_initialization (__main__.TestBinaryLogisticRegression.test_invalid_initialization)\n",
      "Test invalid parameters during initialization. ... ok\n",
      "test_invalid_loss_inputs (__main__.TestBinaryLogisticRegression.test_invalid_loss_inputs)\n",
      "Test invalid inputs for the loss function. ... ok\n",
      "test_invalid_predict_inputs (__main__.TestBinaryLogisticRegression.test_invalid_predict_inputs)\n",
      "Test invalid prediction inputs for X. ... ok\n",
      "test_invalid_train_inputs (__main__.TestBinaryLogisticRegression.test_invalid_train_inputs)\n",
      "Test invalid training inputs for X and Y. ... ok\n",
      "test_loss_initialization (__main__.TestBinaryLogisticRegression.test_loss_initialization)\n",
      "Test that the initial loss is computed correctly. ... ok\n",
      "test_new_unseen_data (__main__.TestBinaryLogisticRegression.test_new_unseen_data)\n",
      "Test the model on unseen data after training. ... ok\n",
      "test_non_separable_data (__main__.TestBinaryLogisticRegression.test_non_separable_data)\n",
      "Test behavior with non-linearly separable data. ... ok\n",
      "test_sigmoid (__main__.TestBinaryLogisticRegression.test_sigmoid)\n",
      "Test that sigmoid outputs correct calculations. ... ok\n",
      "test_training_and_predictions (__main__.TestBinaryLogisticRegression.test_training_and_predictions)\n",
      "Test that the model learns correctly on training data. ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 28 tests in 0.354s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class TestAllPairsLogisticRegression(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \"\"\"Initialize common test parameters.\"\"\"\n",
    "        self.n_classes = 3\n",
    "        self.n_features = 2\n",
    "        self.batch_size = 1\n",
    "        self.model = AllPairsLogisticRegression(\n",
    "            n_classes=self.n_classes,\n",
    "            binary_classifier_class=BinaryLogisticRegression,\n",
    "            n_features=self.n_features,\n",
    "            batch_size=self.batch_size,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_all_classifiers_trained(self):\n",
    "        \"\"\"Test that all required binary classifiers are trained.\"\"\"\n",
    "        X = np.array([[1, 0], [0, 1], [-1, 0], [2, 0]])  # Data separable\n",
    "        Y = np.array([2, 1, 0, 2])  # 3 classes: 0, 1, 2\n",
    "        self.model.train(X, Y)\n",
    "        unique_pairs = [(i, j) for i in range(3) for j in range(3) if i < j]\n",
    "        self.assertEqual(len(self.model.classifiers), len(unique_pairs))\n",
    "        \n",
    "    def test_train_function(self):\n",
    "        \"\"\"Test the train function.\"\"\"\n",
    "        X = np.array([[1, 0], [0, 1], [-1, 0], [2, 0]])  # Data separable\n",
    "        Y = np.array([2, 1, 0, 2])  # 3 classes: 0, 1, 2\n",
    "        self.model.train(X, Y)\n",
    "        self.assertTrue(len(self.model.classifiers) > 0)\n",
    "\n",
    "\n",
    "    def test_train_creates_correct_classifiers_all_pairs(self):\n",
    "        \"\"\"Test that `train` creates one classifier for each pair of classes and trains it correctly.\"\"\"\n",
    "        X = np.array([[1, 0], [0, 1], [-1, 0], [2, 0]])  # Data separable\n",
    "        Y = np.array([2, 1, 0, 2])  # 3 classes: 0, 1, 2\n",
    "\n",
    "        # Train the model\n",
    "        self.model.train(X, Y)\n",
    "\n",
    "        # Check that the correct number of classifiers was created\n",
    "        n_classes = len(np.unique(Y))\n",
    "        expected_classifiers = n_classes * (n_classes - 1) // 2  # Number of class pairs\n",
    "        self.assertEqual(len(self.model.classifiers), expected_classifiers)\n",
    "\n",
    "        # Check each classifier's training data and predictions\n",
    "        for class_i in range(n_classes):\n",
    "            for class_j in range(class_i + 1, n_classes):\n",
    "                # Get the binary classifier for this class pair\n",
    "                classifier = self.model.classifiers[(class_i, class_j)]\n",
    "\n",
    "                # Filter data for classes class_i and class_j\n",
    "                mask = (Y == class_i) | (Y == class_j)\n",
    "                X_pair = X[mask]\n",
    "                Y_pair = Y[mask]\n",
    "\n",
    "                # Convert labels to binary: class_i -> 1, class_j -> 0\n",
    "                binary_labels = np.where(Y_pair == class_i, 1, 0)\n",
    "\n",
    "                # Ensure the classifier's predictions match the binary labels\n",
    "                predictions = classifier.predict(X_pair)\n",
    "                np.testing.assert_array_equal(predictions, binary_labels)\n",
    "\n",
    "\n",
    "    def test_accuracy(self):\n",
    "        \"\"\"Test the accuracy calculation on training data.\"\"\"\n",
    "        X = np.array([[1, 0], [0, 1], [-1, 0], [2, 0]])  # Data separable\n",
    "        Y = np.array([2, 1, 0, 2])  # 3 classes: 0, 1, 2\n",
    "\n",
    "        # Train the model\n",
    "        self.model.train(X, Y)\n",
    "        accuracy = self.model.accuracy(X, Y)\n",
    "        self.assertAlmostEqual(accuracy, 1.0, places=2)\n",
    "\n",
    "    def test_predict_on_unseen_data(self):\n",
    "        \"\"\"Test predictions on unseen testing data.\"\"\"\n",
    "        X_train = np.array([[1, 1], [-1, 1], [1, -1], [-1, -1]])\n",
    "        Y_train = np.array([0, 1, 2, 1])  # 3 classes: 0, 1, 2\n",
    "\n",
    "        X_test = np.array([[2, 2], [-2, -2], [3, 4], [8, -10]])\n",
    "        Y_test = np.array([0, 1, 0, 2])  # Test on similar data\n",
    "\n",
    "        # Train the model\n",
    "        self.model.train(X_train, Y_train)\n",
    "\n",
    "        # Predict on unseen data\n",
    "        predictions = self.model.predict(X_test)\n",
    "\n",
    "        # Check if predictions match true labels\n",
    "        np.testing.assert_array_equal(predictions, Y_test)\n",
    "\n",
    "\n",
    "    def test_train_empty_data(self):\n",
    "        \"\"\"Test that training with empty data raises an error.\"\"\"\n",
    "        X_empty = np.array([])\n",
    "        Y_empty = np.array([])\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.model.train(X_empty, Y_empty)\n",
    "            \n",
    "        \n",
    "    def test_train_dimension_mismatch(self):\n",
    "        \"\"\"Test that training with mismatched dimensions raises an error.\"\"\"\n",
    "        X_mismatch = np.array([[1, 2], [3, 4]])  \n",
    "        Y_mismatch = np.array([0])  \n",
    "        with self.assertRaises(ValueError):\n",
    "            self.model.train(X_mismatch, Y_mismatch)\n",
    "\n",
    "    def test_invalid_binary_classifier_class(self):\n",
    "        \"\"\"Test that an invalid `binary_classifier_class` raises an error.\"\"\"\n",
    "        with self.assertRaises(TypeError):\n",
    "            AllPairsLogisticRegression(\n",
    "                n_classes=3,\n",
    "                n_features=2,\n",
    "                batch_size=1,\n",
    "                epochs=100,\n",
    "                binary_classifier_class=\"NotAClass\",\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "    def test_non_separable_data(self):\n",
    "        \"\"\"Test the model on non-linearly separable data.\"\"\"\n",
    "        X_non_separable = np.array([\n",
    "            [1, 2], [2, 1], [2, 2],\n",
    "            [3, 4], [4, 3], [4, 4],\n",
    "            [5, 6], [6, 5], [6, 6]\n",
    "        ])\n",
    "        Y_non_separable = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]) \n",
    "        self.model.train(X_non_separable, Y_non_separable)\n",
    "        accuracy = self.model.accuracy(X_non_separable, Y_non_separable)\n",
    "        self.assertGreaterEqual(accuracy, 0.5)  \n",
    "    \n",
    "    # # -----------------\n",
    "    # # Additional Tests\n",
    "    # # -----------------\n",
    "\n",
    "    def test_predict_correct_classes(self):\n",
    "        \"\"\"Test that `predict` returns the correct class labels.\"\"\"\n",
    "        X = np.array([[1, 1], [-1, 1], [1, -1], [-1, -1]])\n",
    "        Y = np.array([0, 1, 2, 1]) \n",
    "\n",
    "        self.model.train(X, Y)\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "\n",
    "        np.testing.assert_array_equal(predictions, Y)\n",
    "\n",
    "    def test_accuracy_calculation(self):\n",
    "        \"\"\"Test that `accuracy` computes the correct value.\"\"\"\n",
    "        X = np.array([[1, 1], [-1, 1], [1, -1], [-1, -1]])\n",
    "        Y = np.array([0, 1, 2, 1])  \n",
    "\n",
    "        self.model.train(X, Y)\n",
    "\n",
    "        acc = self.model.accuracy(X, Y)\n",
    "\n",
    "        self.assertAlmostEqual(acc, 1.0)\n",
    "\n",
    "    def test_invalid_n_classes(self):\n",
    "        \"\"\"Test that an invalid `n_classes` parameter raises an error.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            AllPairsLogisticRegression(\n",
    "                n_classes=0,  # Invalid\n",
    "                binary_classifier_class=BinaryLogisticRegression,\n",
    "                n_features=2,\n",
    "                batch_size=1,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "    def test_invalid_n_features(self):\n",
    "        \"\"\"Test that an invalid `n_features` parameter raises an error.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            AllPairsLogisticRegression(\n",
    "                n_classes=3,\n",
    "                binary_classifier_class=BinaryLogisticRegression,\n",
    "                n_features=-1,  # Invalid\n",
    "                batch_size=1,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "    def test_invalid_batch_size(self):\n",
    "        \"\"\"Test that an invalid `batch_size` parameter raises an error.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            AllPairsLogisticRegression(\n",
    "                n_classes=3,\n",
    "                binary_classifier_class=BinaryLogisticRegression,\n",
    "                n_features=2,\n",
    "                batch_size=0,  # Invalid\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "    def test_invalid_epochs(self):\n",
    "        \"\"\"Test that an invalid `max_epochs` parameter raises an error.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            AllPairsLogisticRegression(\n",
    "                n_classes=3,\n",
    "                binary_classifier_class=BinaryLogisticRegression,\n",
    "                n_features=2,\n",
    "                batch_size=1,\n",
    "                max_epochs=-10,  # Invalid\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "    def test_train_empty_data(self):\n",
    "        \"\"\"Test that training with empty data raises an error.\"\"\"\n",
    "        X = np.array([])\n",
    "        Y = np.array([])\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.model.train(X, Y)\n",
    "\n",
    "\n",
    "    def test_predict_invalid_dimensions(self):\n",
    "        \"\"\"Test that `predict` with invalid dimensions raises an error.\"\"\"\n",
    "        X = np.array([[1, 2, 3]])  \n",
    "        with self.assertRaises(ValueError):\n",
    "            self.model.predict(X)\n",
    "    \n",
    "    def test_train_dimension_mismatch(self):\n",
    "        \"\"\"Test that training with mismatched dimensions raises an error.\"\"\"\n",
    "        X = np.array([[1, 2], [3, 4]])\n",
    "        Y = np.array([0])  # size mismatch\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.model.train(X, Y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use `unittest.main()` with arguments to avoid conflicts with Jupyter Notebook.\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test One-vs-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_accuracy (__main__.TestAllPairsLogisticRegression.test_accuracy)\n",
      "Test the accuracy calculation on training data. ... ok\n",
      "test_accuracy_calculation (__main__.TestAllPairsLogisticRegression.test_accuracy_calculation)\n",
      "Test that `accuracy` computes the correct value. ... ok\n",
      "test_all_classifiers_trained (__main__.TestAllPairsLogisticRegression.test_all_classifiers_trained)\n",
      "Test that all required binary classifiers are trained. ... ok\n",
      "test_invalid_batch_size (__main__.TestAllPairsLogisticRegression.test_invalid_batch_size)\n",
      "Test that an invalid `batch_size` parameter raises an error. ... ok\n",
      "test_invalid_binary_classifier_class (__main__.TestAllPairsLogisticRegression.test_invalid_binary_classifier_class)\n",
      "Test that an invalid `binary_classifier_class` raises an error. ... ok\n",
      "test_invalid_epochs (__main__.TestAllPairsLogisticRegression.test_invalid_epochs)\n",
      "Test that an invalid `max_epochs` parameter raises an error. ... ok\n",
      "test_invalid_n_classes (__main__.TestAllPairsLogisticRegression.test_invalid_n_classes)\n",
      "Test that an invalid `n_classes` parameter raises an error. ... ok\n",
      "test_invalid_n_features (__main__.TestAllPairsLogisticRegression.test_invalid_n_features)\n",
      "Test that an invalid `n_features` parameter raises an error. ... ok\n",
      "test_non_separable_data (__main__.TestAllPairsLogisticRegression.test_non_separable_data)\n",
      "Test the model on non-linearly separable data. ... ok\n",
      "test_predict_correct_classes (__main__.TestAllPairsLogisticRegression.test_predict_correct_classes)\n",
      "Test that `predict` returns the correct class labels. ... ok\n",
      "test_predict_invalid_dimensions (__main__.TestAllPairsLogisticRegression.test_predict_invalid_dimensions)\n",
      "Test that `predict` with invalid dimensions raises an error. ... ok\n",
      "test_predict_on_unseen_data (__main__.TestAllPairsLogisticRegression.test_predict_on_unseen_data)\n",
      "Test predictions on unseen testing data. ... ok\n",
      "test_train_creates_correct_classifiers_all_pairs (__main__.TestAllPairsLogisticRegression.test_train_creates_correct_classifiers_all_pairs)\n",
      "Test that `train` creates one classifier for each pair of classes and trains it correctly. ... ok\n",
      "test_train_dimension_mismatch (__main__.TestAllPairsLogisticRegression.test_train_dimension_mismatch)\n",
      "Test that training with mismatched dimensions raises an error. ... ok\n",
      "test_train_empty_data (__main__.TestAllPairsLogisticRegression.test_train_empty_data)\n",
      "Test that training with empty data raises an error. ... ok\n",
      "test_train_function (__main__.TestAllPairsLogisticRegression.test_train_function)\n",
      "Test the train function. ... ok\n",
      "test_gradient_calculation (__main__.TestBinaryLogisticRegression.test_gradient_calculation)\n",
      "Test that the gradient is computed correctly. ... ok\n",
      "test_imbalanced_data (__main__.TestBinaryLogisticRegression.test_imbalanced_data)\n",
      "Test behavior on an imbalanced dataset. ... ok\n",
      "test_invalid_accuracy_inputs (__main__.TestBinaryLogisticRegression.test_invalid_accuracy_inputs)\n",
      "Test invalid inputs for accuracy calculation. ... ok\n",
      "test_invalid_initialization (__main__.TestBinaryLogisticRegression.test_invalid_initialization)\n",
      "Test invalid parameters during initialization. ... ok\n",
      "test_invalid_loss_inputs (__main__.TestBinaryLogisticRegression.test_invalid_loss_inputs)\n",
      "Test invalid inputs for the loss function. ... ok\n",
      "test_invalid_predict_inputs (__main__.TestBinaryLogisticRegression.test_invalid_predict_inputs)\n",
      "Test invalid prediction inputs for X. ... ok\n",
      "test_invalid_train_inputs (__main__.TestBinaryLogisticRegression.test_invalid_train_inputs)\n",
      "Test invalid training inputs for X and Y. ... ok\n",
      "test_loss_initialization (__main__.TestBinaryLogisticRegression.test_loss_initialization)\n",
      "Test that the initial loss is computed correctly. ... ok\n",
      "test_new_unseen_data (__main__.TestBinaryLogisticRegression.test_new_unseen_data)\n",
      "Test the model on unseen data after training. ... ok\n",
      "test_non_separable_data (__main__.TestBinaryLogisticRegression.test_non_separable_data)\n",
      "Test behavior with non-linearly separable data. ... ok\n",
      "test_sigmoid (__main__.TestBinaryLogisticRegression.test_sigmoid)\n",
      "Test that sigmoid outputs correct calculations. ... ok\n",
      "test_training_and_predictions (__main__.TestBinaryLogisticRegression.test_training_and_predictions)\n",
      "Test that the model learns correctly on training data. ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 28 tests in 0.322s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OneVsAllLogisticRegression:\n",
    "    def __init__(self, n_classes, binary_classifier_class, n_features, batch_size, max_epochs = 100, conv_threshold = 1e-6, random_state = None):\n",
    "        \"\"\"\n",
    "        Initialize the One-vs-All logistic regression model.\n",
    "        @param n_classes: Number of classes in the dataset, an integer.\n",
    "        @param binary_classifier_class: Class for binary logistic regression, a class object.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training the binary classifiers, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(n_classes, int) or n_classes <= 1:\n",
    "            raise ValueError(\"`n_classes` must be an integer greater than 1.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "            raise ValueError(\"`epochs` must be an integer greater than 0.\")\n",
    "        if not callable(binary_classifier_class):\n",
    "            raise TypeError(\"`binary_classifier_class` must be a callable class.\")\n",
    "        if not isinstance(n_features, int) or n_features <= 0:\n",
    "            raise ValueError(\"`n_features` must be a positive integer.\")\n",
    "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "            raise ValueError(\"`batch_size` must be a positive integer.\")\n",
    "        if not isinstance(max_epochs, int) or max_epochs <= 0:\n",
    "                raise ValueError(\"`max_epochs` must be a positive number.\")\n",
    "        if not isinstance(conv_threshold, (int, float)) or conv_threshold <= 0:\n",
    "            raise ValueError(\"`conv_threshold` must be a positive number.\")\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.classifiers = {}  \n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs \n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.binary_classifier_class = binary_classifier_class\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the One-vs-All logistic regression model by training one binary classifier\n",
    "        for each class in the dataset.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: Labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"Input data `X` and labels `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Mismatch in number of samples between `X` and `Y`.\")\n",
    "        if np.any((Y < 0) | (Y >= self.n_classes)):\n",
    "            raise ValueError(f\"Labels in `Y` must be in the range [0, {self.n_classes - 1}].\")\n",
    "        \n",
    "        for class_i in range(self.n_classes):\n",
    "            # Create binary labels: 1 for the current class, 0 for others\n",
    "            binary_labels = np.where(Y == class_i, 1, 0)\n",
    "            classifier = self.binary_classifier_class(\n",
    "                n_features=self.n_features,\n",
    "                batch_size=self.batch_size,\n",
    "                max_epochs=self.max_epochs, random_state=self.random_state,\n",
    "                conv_threshold = self.conv_threshold\n",
    "            )\n",
    "            classifier.train(X, binary_labels)\n",
    "            self.classifiers[class_i] = classifier\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data using the trained classifiers.\n",
    "        @param X: Input features to classify, a numpy array of shape (n_samples, n_features).\n",
    "        @return: Predicted class labels, a numpy array of shape (n_samples,).\n",
    "        \"\"\"\n",
    "\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Input data `X` cannot be empty.\")\n",
    "        if X.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"`X` must have {self.n_features} features.\")\n",
    "\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        scores = np.zeros((n_samples, self.n_classes))\n",
    "\n",
    "        for class_i, classifier in self.classifiers.items():\n",
    "            # Get probabilities for the current class\n",
    "            scores[:, class_i] = classifier.predict_probs(X)\n",
    "\n",
    "        # Select the class with the highest probability/score for each sample\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data and labels.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: True labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: Accuracy of the model as a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise ValueError(\"Input data `X` and labels `Y` cannot be empty.\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Mismatch in number of samples between `X` and `Y`.\")\n",
    "        \n",
    "        preds = self.predict(X)\n",
    "        acc = np.mean(preds == Y)\n",
    "        return acc\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use `unittest.main()` with arguments to avoid conflicts with Jupyter Notebook.\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Work: Scikit-learn & Kaggle Notebook\n",
    "\n",
    "Our project is attempting to reimplement scikit-learn's One-vs-all and All-pairs algorithms using Binary Logistic Regression. We found a publicly available Kaggle notebook for our dataset by a data scientist, Mr Amine (Boudinar, 2023), that implements this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/musatahir/CS2060/Multiclass-Classification-Algorithm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>SCC</th>\n",
       "      <th>MTRANS_Walking</th>\n",
       "      <th>FAVC_z</th>\n",
       "      <th>FCVC_minmax</th>\n",
       "      <th>NCP_z</th>\n",
       "      <th>CAEC_minmax</th>\n",
       "      <th>CH2O_minmax</th>\n",
       "      <th>FAF_minmax</th>\n",
       "      <th>TUE_z</th>\n",
       "      <th>CALC_z</th>\n",
       "      <th>Age_bin_minmax</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.766876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.404704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550985</td>\n",
       "      <td>1.439033</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.766876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.092724</td>\n",
       "      <td>0.516552</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.766876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.404704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.550985</td>\n",
       "      <td>2.472136</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.766876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.092724</td>\n",
       "      <td>2.472136</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.766876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.164116</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.092724</td>\n",
       "      <td>0.516552</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height  Weight  family_history_with_overweight  SCC  MTRANS_Walking  \\\n",
       "0    1.62    64.0                               1    0               0   \n",
       "1    1.52    56.0                               1    1               0   \n",
       "2    1.80    77.0                               1    0               0   \n",
       "3    1.80    87.0                               0    0               1   \n",
       "4    1.78    89.8                               0    0               0   \n",
       "\n",
       "     FAVC_z  FCVC_minmax     NCP_z  CAEC_minmax  CH2O_minmax  FAF_minmax  \\\n",
       "0  2.766876          0.5  0.404704     0.333333          0.5    0.000000   \n",
       "1  2.766876          1.0  0.404704     0.333333          1.0    1.000000   \n",
       "2  2.766876          0.5  0.404704     0.333333          0.5    0.666667   \n",
       "3  2.766876          1.0  0.404704     0.333333          0.5    0.666667   \n",
       "4  2.766876          0.5  2.164116     0.333333          0.5    0.000000   \n",
       "\n",
       "      TUE_z    CALC_z  Age_bin_minmax  NObeyesdad  \n",
       "0  0.550985  1.439033            0.25           1  \n",
       "1  1.092724  0.516552            0.25           1  \n",
       "2  0.550985  2.472136            0.50           1  \n",
       "3  1.092724  2.472136            0.75           2  \n",
       "4  1.092724  0.516552            0.50           3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os \n",
    "par_working_dir = \"/Users/musatahir/CS2060/Multiclass-Classification-Algorithm\"\n",
    "os.chdir(par_working_dir)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "data = pd.read_csv(\"./data/raw/obesity_dataset.csv\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations and References \n",
    "- Boudinar, A. (2023) Obesity risk prediction. Available at: https://www.kaggle.com/code/boudinar2001/obesity-risk-prediction (Accessed: [Insert date here]).\n",
    "- Pawara, P., Okafor, E., Groefsema, M., He, S., Schomaker, L.R.B. and Wiering, M.A. (2020). One-vs-One classification for deep neural networks. Pattern Recognition, 108, p.107528. doi:https://doi.org/10.1016/j.patcog.2020.107528.\n",
    "- Rifkin, R. and Klautau, A. (2004). In Defense of One-Vs-All Classification. Journal of Machine Learning Research, [online] 5, pp.101–141. Available at: https://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
