{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 186,
=======
   "execution_count": 17,
>>>>>>> 21e9e41 (tests)
   "id": "79e49c5e-1b2a-49d8-9d65-20ba40c2b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, n_features, batch_size, epochs):\n",
    "        \"\"\"Initialize the binary logistic regression model.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros(n_features + 1)  # extra element for bias\n",
    "        self.alpha = 0.05\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Perform sigmoid operation\n",
    "        @params:\n",
    "            z: the input to which sigmoid will be applied\n",
    "        @return:\n",
    "            an array with sigmoid applied elementwise.\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "        # intializing values\n",
    "        converge = False\n",
    "        epochs = 0\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        for epoch in self.epochs:\n",
    "            # update # of epochs\n",
    "            # acquire indices for shuffling of X and Y\n",
    "            indices = np.arange(n_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "            # calc last epoch loss\n",
    "            last_epoch_loss = self.loss(X, Y)\n",
    "            # for the # of batches\n",
    "            for i in range(0, n_examples, self.batch_size):\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                Y_batch = Y[i:i + self.batch_size]\n",
    "                # reinitialize gradient to be 0s\n",
    "                grad = np.zeros(self.weights.shape)\n",
    "                # for each pair in the batch\n",
    "                for x, y in zip(X_batch, Y_batch):\n",
    "                    prediction = self.sigmoid(np.dot(self.weights, x))\n",
    "                    # gradient calculation\n",
    "                    error = prediction - y\n",
    "                    grad += error * x\n",
    "                # update weights\n",
    "                self.weights -= ((self.alpha * grad)/ self.batch_size)\n",
    "            epoch_loss = self.loss(X, Y)\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        n_examples = X.shape[0]\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(n_examples):\n",
    "            # linear output (dot product)\n",
    "            linear_output = np.dot(self.weights, X[i])\n",
    "            # calc logistic loss for each sample\n",
    "            y = 1 if Y[i] == 1 else -1\n",
    "            logistic_loss = np.log(1 + np.exp(-y * linear_output))\n",
    "            total_loss += logistic_loss\n",
    "    \n",
    "        return total_loss / n_examples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # multiply X by weights of model\n",
    "        predictions = self.sigmoid(X @ self.weights.T)\n",
    "        return np.where(predictions >= 0.5, 1, 0)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 187,
=======
   "execution_count": 18,
>>>>>>> 21e9e41 (tests)
   "id": "546682e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllPairsLogisticRegression:\n",
    "    def __init__(self, n_classes, binary_classifier_class, n_features, batch_size, conv_threshold):\n",
    "        \"\"\"\n",
    "        Initialize the all-pairs logistic regression model.\n",
    "        @param n_classes: Number of classes in the dataset, an integer.\n",
    "        @param binary_classifier_class: Class for binary logistic regression, a class object.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training the binary classifiers, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes\n",
    "        self.classifiers = {} \n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.binary_classifier_class = binary_classifier_class\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the all-pairs logistic regression model by training binary classifiers\n",
    "        for each pair of classes in the dataset.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: Labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        for class_i in range(self.n_classes):\n",
    "            for class_j in range(class_i + 1, self.n_classes):\n",
    "                SX = []\n",
    "                SY = []\n",
    "                for t in range(len(Y)):\n",
    "                    if Y[t] == class_i:\n",
    "                        SX.append(X[t])\n",
    "                        SY.append(1)\n",
    "                    elif Y[t] == class_j:\n",
    "                        SX.append(X[t])\n",
    "                        SY.append(-1)\n",
    "                SX = np.array(SX)\n",
    "                SY = np.array(SY)\n",
    "                SX = np.hstack([SX, np.ones((SX.shape[0], 1))])\n",
    "                classifier = self.binary_classifier_class(\n",
    "                    n_features=self.n_features,\n",
    "                    batch_size=self.batch_size,\n",
    "                    conv_threshold=self.conv_threshold\n",
    "                )\n",
    "                classifier.train(SX, SY)\n",
    "                self.classifiers[(class_i, class_j)] = classifier\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data using the trained classifiers.\n",
    "        @param X: Input features to classify, a numpy array of shape (n_samples, n_features).\n",
    "        @return: Predicted class labels, a numpy array of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        votes = np.zeros((n_samples, self.n_classes), dtype=int)\n",
    "        for (class_i, class_j), classifier in self.classifiers.items():\n",
    "            predictions = classifier.predict(X)\n",
    "            votes[:, class_i] += (predictions == 1)\n",
    "            votes[:, class_j] += (predictions == 0)\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data and labels.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: True labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: Accuracy of the model as a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        correct_predictions = np.sum(predictions == Y)\n",
    "        return correct_predictions / len(Y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 188,
=======
   "execution_count": 19,
>>>>>>> 21e9e41 (tests)
   "id": "a5fe8277-9c63-4cc7-a0c0-bb0fb7a5d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import random\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# BINARY LOGISTIC REGRESSION\n",
    "# test model with 1 predictor, batch size of 1 and conv threshold of 1e-2 (only 2 classes bc binary)\n",
    "test_model1 = BinaryLogisticRegression(n_features=1, batch_size=1, conv_threshold=1e-3)\n",
    "\n",
    "# test data with bias term\n",
    "x_bias = np.array([[1, 1], [2, 1], [3, 1], [4, 1], [5, 1], [1.2, 1]])  \n",
    "# labels\n",
    "y = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "# calc init loss\n",
    "initial_loss = test_model1.loss(x_bias, y)\n",
    "assert initial_loss == pytest.approx(0.693, 0.001)\n",
    "\n",
    "# checking that weights have the correct shape\n",
    "assert test_model1.weights.shape == (2,)\n",
    "\n",
    "# train model\n",
    "test_model1.train(x_bias, y)\n",
    "\n",
    "# test model by inputting training data --> accuracy should be 100%\n",
    "x_bias_test = np.array([[1, 1], [2, 1], [3, 1], [4, 1], [5, 1], [1.2, 1]])\n",
    "predictions = test_model1.predict(x_bias_test)\n",
    "expected_predictions = np.array([0, 0, 1, 1, 1, 0])\n",
    "assert np.all(predictions == expected_predictions)\n",
    "accuracy = test_model1.accuracy(x_bias_test, expected_predictions)\n",
    "assert accuracy == pytest.approx(1.0, 0.01)\n",
    "\n",
    "# input new unseen testing data --> accuracy should also be 100%\n",
    "x_bias_test = np.array([[1.5, 1], [3.5, 1]])\n",
    "predictions = test_model1.predict(x_bias_test)\n",
    "expected_predictions = np.array([0, 1]) \n",
    "assert np.all(predictions == expected_predictions)\n",
    "accuracy = test_model1.accuracy(x_bias_test, expected_predictions)\n",
    "assert accuracy == pytest.approx(1.0, 0.01)\n",
    "\n",
    "# testing weight calculations manually [as implemented in the code]\n",
    "weights = np.zeros(2)  \n",
    "x_sample = np.array([1, 1])  \n",
    "y_sample = 0 \n",
    "z = np.dot(weights, x_sample)\n",
    "prediction = 1 / (1 + np.exp(-z))\n",
    "gradient = (prediction - y_sample) * x_sample\n",
    "test_gradient = np.array([0.5, 0.5])  \n",
    "assert gradient == pytest.approx(test_gradient, 0.01)\n",
    "\n",
    "# testing case with one data point and testing to see behavior of weights\n",
    "test_model2 = BinaryLogisticRegression(n_features=1, batch_size=1, conv_threshold=1e-2)\n",
    "x_train = np.array([[1, 1]])  \n",
    "y_train = np.array([1])       \n",
    "test_model2.train(x_train, y_train)\n",
    "assert test_model2.weights[0] > 0 # positive\n",
    "assert test_model2.weights[1] > 0 # bias also positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "842ec9a3-b05e-4235-bb7a-e10ebfa5a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/k38qrhtj2sg_22kzbmh35pm40000gp/T/ipykernel_57510/184615367.py:25: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/v8/k38qrhtj2sg_22kzbmh35pm40000gp/T/ipykernel_57510/184615367.py:87: RuntimeWarning: overflow encountered in exp\n",
      "  logistic_loss = np.log(1 + np.exp(-y * linear_output))\n",
      "/var/folders/v8/k38qrhtj2sg_22kzbmh35pm40000gp/T/ipykernel_57510/184615367.py:66: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  if abs(epoch_loss - last_epoch_loss) < self.conv_threshold:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# check number of classifiers trained \u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtest_model_ap1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_model_ap1\u001b[38;5;241m.\u001b[39mclassifiers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# check classifiers use binary logistic regression\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 46\u001b[0m, in \u001b[0;36mAllPairsLogisticRegression.train\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     40\u001b[0m SX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([SX, np\u001b[38;5;241m.\u001b[39mones((SX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))])\n\u001b[1;32m     41\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_classifier_class(\n\u001b[1;32m     42\u001b[0m     n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features,\n\u001b[1;32m     43\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     44\u001b[0m     conv_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_threshold\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifiers[(class_i, class_j)] \u001b[38;5;241m=\u001b[39m classifier\n",
      "Cell \u001b[0;32mIn[17], line 65\u001b[0m, in \u001b[0;36mBinaryLogisticRegression.train\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m grad)\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m---> 65\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(epoch_loss \u001b[38;5;241m-\u001b[39m last_epoch_loss) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_threshold:\n\u001b[1;32m     67\u001b[0m     converge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 87\u001b[0m, in \u001b[0;36mBinaryLogisticRegression.loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# calc logistic loss for each sample\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Y[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 87\u001b[0m     logistic_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlinear_output\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     88\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logistic_loss\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m n_examples\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ALL PAIRS LOGISTIC REGRESSION\n",
    "test_model_ap1 = AllPairsLogisticRegression(\n",
    "    n_classes=3,\n",
    "    binary_classifier_class=BinaryLogisticRegression,\n",
    "    n_features=4,\n",
    "    batch_size=32,\n",
    "    conv_threshold=1e-2\n",
    ")\n",
    "assert test_model_ap1.n_classes == 3\n",
    "assert test_model_ap1.n_features == 4\n",
    "assert test_model_ap1.batch_size == 32\n",
    "assert test_model_ap1.conv_threshold == 1e-2\n",
    "assert test_model_ap1.classifiers == {}\n",
    "\n",
    "# synthetic training data\n",
    "X_train = np.array([\n",
    "    [1, 2, 3, 4],  # Class 0\n",
    "    [5, 6, 7, 8],  # Class 0\n",
    "    [9, 10, 11, 12],  # Class 1\n",
    "    [13, 14, 15, 16],  # Class 1\n",
    "    [17, 18, 19, 20],  # Class 2\n",
    "    [21, 22, 23, 24]   # Class 2\n",
    "])\n",
    "Y_train = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "# check number of classifiers trained \n",
    "test_model_ap1.train(X_train, Y_train)\n",
    "assert len(test_model_ap1.classifiers) == 3\n",
    "\n",
    "# check classifiers use binary logistic regression\n",
    "for key in test_model_ap1.classifiers:\n",
    "    assert isinstance(test_model_ap1.classifiers[key], BinaryLogisticRegression)\n",
    "\n",
    "# test predict function\n",
    "X_test = np.array([\n",
    "    [2, 3, 4, 5], \n",
    "    [10, 11, 12, 13], \n",
    "    [18, 19, 20, 21]   \n",
    "])\n",
    "Y_test = np.array([0, 1, 2])\n",
    "\n",
    "predictions = test_model_ap1.predict(X_test)\n",
    "expected_predictions = np.array([0, 1, 2])\n",
    "assert np.all(predictions == expected_predictions)\n",
    "\n",
    "# Test accuracy\n",
    "accuracy = test_model_ap1.accuracy(X_test, Y_test)\n",
    "assert accuracy == pytest.approx(1.0, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
