{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "79e49c5e-1b2a-49d8-9d65-20ba40c2b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, n_features, batch_size, conv_threshold):\n",
    "        \"\"\"Initialize the binary logistic regression model.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros(n_features + 1)  # extra element for bias\n",
    "        self.alpha = 0.03\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Perform sigmoid operation\n",
    "        @params:\n",
    "            z: the input to which sigmoid will be applied\n",
    "        @return:\n",
    "            an array with sigmoid applied elementwise.\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "        # intializing values\n",
    "        converge = False\n",
    "        epochs = 0\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        while not converge:\n",
    "            # update # of epochs\n",
    "            epochs += 1\n",
    "            # acquire indices for shuffling of X and Y\n",
    "            indices = np.arange(n_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "            # calc last epoch loss\n",
    "            last_epoch_loss = self.loss(X, Y)\n",
    "            # for the # of batches\n",
    "            for i in range(0, n_examples, self.batch_size):\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                Y_batch = Y[i:i + self.batch_size]\n",
    "                # reinitialize gradient to be 0s\n",
    "                grad = np.zeros(self.weights.shape)\n",
    "                # for each pair in the batch\n",
    "                for x, y in zip(X_batch, Y_batch):\n",
    "                    prediction = self.sigmoid(np.dot(self.weights, x))\n",
    "                    # gradient calculation\n",
    "                    error = prediction - y\n",
    "                    grad += error * x\n",
    "                # update weights\n",
    "                self.weights -= ((self.alpha * grad)/ self.batch_size)\n",
    "            epoch_loss = self.loss(X, Y)\n",
    "            if abs(epoch_loss - last_epoch_loss) < self.conv_threshold:\n",
    "                converge = True\n",
    "        return epochs\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        n_examples = X.shape[0]\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(n_examples):\n",
    "            # linear output (dot product)\n",
    "            linear_output = np.dot(self.weights, X[i])\n",
    "            # calc logistic loss for each sample\n",
    "            y = 1 if Y[i] == 1 else -1\n",
    "            logistic_loss = np.log(1 + np.exp(-y * linear_output))\n",
    "            total_loss += logistic_loss\n",
    "    \n",
    "        return total_loss / n_examples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # multiply X by weights of model\n",
    "        predictions = self.sigmoid(X @ self.weights)\n",
    "        return np.where(predictions >= 0.5, 1, 0)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "546682e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllPairsLogisticRegression:\n",
    "    def __init__(self, n_classes, binary_classifier_class, n_features, batch_size, conv_threshold):\n",
    "        \"\"\"\n",
    "        Initialize the all-pairs logistic regression model.\n",
    "        @param n_classes: Number of classes in the dataset, an integer.\n",
    "        @param binary_classifier_class: Class for binary logistic regression, a class object.\n",
    "        @param n_features: Number of features in the dataset, an integer.\n",
    "        @param batch_size: Batch size for training the binary classifiers, an integer.\n",
    "        @param conv_threshold: Convergence threshold for training, a float.\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes\n",
    "        self.classifiers = {} \n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.binary_classifier_class = binary_classifier_class\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the all-pairs logistic regression model by training binary classifiers\n",
    "        for each pair of classes in the dataset.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: Labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        for class_i in range(self.n_classes):\n",
    "            for class_j in range(class_i + 1, self.n_classes):\n",
    "                SX = []\n",
    "                SY = []\n",
    "                for t in range(len(Y)):\n",
    "                    if Y[t] == class_i:\n",
    "                        SX.append(X[t])\n",
    "                        SY.append(1)\n",
    "                    elif Y[t] == class_j:\n",
    "                        SX.append(X[t])\n",
    "                        SY.append(-1)\n",
    "                SX = np.array(SX)\n",
    "                SY = np.array(SY)\n",
    "                classifier = self.binary_classifier_class(\n",
    "                    n_features=self.n_features,\n",
    "                    batch_size=self.batch_size,\n",
    "                    conv_threshold=self.conv_threshold\n",
    "                )\n",
    "                classifier.train(SX, SY)\n",
    "                self.classifiers[(class_i, class_j)] = classifier\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data using the trained classifiers.\n",
    "        @param X: Input features to classify, a numpy array of shape (n_samples, n_features).\n",
    "        @return: Predicted class labels, a numpy array of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        votes = np.zeros((n_samples, self.n_classes), dtype=int)\n",
    "        for (class_i, class_j), classifier in self.classifiers.items():\n",
    "            predictions = classifier.predict(X)\n",
    "            votes[:, class_i] += (predictions == 1)\n",
    "            votes[:, class_j] += (predictions == 0)\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data and labels.\n",
    "        @param X: Input features of the dataset, a numpy array of shape (n_samples, n_features).\n",
    "        @param Y: True labels of the dataset, a numpy array of shape (n_samples,).\n",
    "        @return: Accuracy of the model as a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        correct_predictions = np.sum(predictions == Y)\n",
    "        return correct_predictions / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5fe8277-9c63-4cc7-a0c0-bb0fb7a5d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import random\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# BINARY LOGISTIC REGRESSION\n",
    "# test model with 1 predictor, batch size of 1 and conv threshold of 1e-2 (only 2 classes bc binary)\n",
    "test_model1 = BinaryLogisticRegression(n_features=1, batch_size=1, conv_threshold=1e-3)\n",
    "\n",
    "# test data with bias term\n",
    "x_bias = np.array([[1, 1], [2, 1], [3, 1], [4, 1], [5, 1], [1.2, 1]])  \n",
    "# labels\n",
    "y = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "# calc init loss\n",
    "initial_loss = test_model1.loss(x_bias, y)\n",
    "assert initial_loss == pytest.approx(0.693, 0.001)\n",
    "\n",
    "# checking that weights have the correct shape\n",
    "assert test_model1.weights.shape == (2,)\n",
    "\n",
    "# train model\n",
    "test_model1.train(x_bias, y)\n",
    "\n",
    "# test model by inputting training data --> accuracy should be 100%\n",
    "x_bias_test = np.array([[1, 1], [2, 1], [3, 1], [4, 1], [5, 1], [1.2, 1]])\n",
    "predictions = test_model1.predict(x_bias_test)\n",
    "expected_predictions = np.array([0, 0, 1, 1, 1, 0])\n",
    "assert np.all(predictions == expected_predictions)\n",
    "accuracy = test_model1.accuracy(x_bias_test, expected_predictions)\n",
    "assert accuracy == pytest.approx(1.0, 0.01)\n",
    "\n",
    "# input new unseen testing data --> accuracy should also be 100%\n",
    "x_bias_test = np.array([[1.5, 1], [3.5, 1]])\n",
    "predictions = test_model1.predict(x_bias_test)\n",
    "expected_predictions = np.array([0, 1]) \n",
    "assert np.all(predictions == expected_predictions)\n",
    "accuracy = test_model1.accuracy(x_bias_test, expected_predictions)\n",
    "assert accuracy == pytest.approx(1.0, 0.01)\n",
    "\n",
    "# testing weight calculations manually [as implemented in the code]\n",
    "weights = np.zeros(2)  \n",
    "x_sample = np.array([1, 1])  \n",
    "y_sample = 0 \n",
    "z = np.dot(weights, x_sample)\n",
    "prediction = 1 / (1 + np.exp(-z))\n",
    "gradient = (prediction - y_sample) * x_sample\n",
    "test_gradient = np.array([0.5, 0.5])  \n",
    "assert gradient == pytest.approx(test_gradient, 0.01)\n",
    "\n",
    "# testing case with one data point and testing to see behavior of weights\n",
    "test_model2 = BinaryLogisticRegression(n_features=1, batch_size=1, conv_threshold=1e-2)\n",
    "x_train = np.array([[1, 1]])  \n",
    "y_train = np.array([1])       \n",
    "test_model2.train(x_train, y_train)\n",
    "assert test_model3.weights[0] > 0 # positive\n",
    "assert test_model3.weights[1] > 0 # bias also positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ec9a3-b05e-4235-bb7a-e10ebfa5a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
