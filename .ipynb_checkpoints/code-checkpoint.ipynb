{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e49c5e-1b2a-49d8-9d65-20ba40c2b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, n_features, batch_size, conv_threshold):\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros(n_features + 1)  # extra element for bias\n",
    "        self.alpha = 0.01\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # intializing values\n",
    "        converge = False\n",
    "        epochs = 0\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        while not converge:\n",
    "            # update # of epochs\n",
    "            epochs += 1\n",
    "            # acquire indices for shuffling of X and Y\n",
    "            indices = np.arange(n_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "            # calc last epoch loss\n",
    "            last_epoch_loss = self.loss(X, Y)\n",
    "            # for the # of batches\n",
    "            for i in range(0, n_examples, self.batch_size):\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                Y_batch = Y[i:i + self.batch_size]\n",
    "                # reinitialize gradient to be 0s\n",
    "                grad = np.zeros(self.weights.shape)\n",
    "                # for each pair in the batch\n",
    "                for x, y in zip(X_batch, Y_batch):\n",
    "                    prediction = self.sigmoid(np.dot(self.weights, x))\n",
    "                    # gradient calculation\n",
    "                    error = prediction - y\n",
    "                    grad += error * x\n",
    "                # update weights\n",
    "                self.weights -= (self.alpha * grad) / self.batch_size\n",
    "            epoch_loss = self.loss(X, Y)\n",
    "            if abs(epoch_loss - last_epoch_loss) < self.conv_threshold:\n",
    "                converge = True\n",
    "        return epochs\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        n_examples = X.shape[0]\n",
    "        total_loss = 0\n",
    "    \n",
    "        for i in range(n_examples):\n",
    "            linear_output = np.dot(self.weights, X[i])\n",
    "            y = 1 if Y[i] == 1 else -1  # Convert labels to {-1, 1}\n",
    "            # compute logistic loss\n",
    "            logistic_loss = np.log(1 + np.exp(-y * linear_output))\n",
    "            total_loss += logistic_loss\n",
    "    \n",
    "        return total_loss / n_examples\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # multiply X by weights of model\n",
    "        predictions = self.sigmoid(X @ self.weights)\n",
    "        return np.where(predictions >= 0.5, 1, 0)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
