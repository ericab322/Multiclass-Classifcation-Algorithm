{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b83161-d6ea-4ae5-b36f-9457f6c20ec3",
   "metadata": {},
   "source": [
    "Logistic Regression uses the sigmoid function, which is defined as follows:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where z is $\\langle w,x \\rangle$\n",
    "This function takes in values of $X = \\mathbb{R}^d$ and outputs continuous values in [0,1] that correspond to probabilities that are used to classify the points as $Y$ = {1, -1}.\n",
    "\n",
    "The decision boundary based on this classifier is still $\\langle w,x \\rangle = 0$ and corresponds to a probability of 50%.\n",
    "\n",
    "Now moving on to the loss for logisitic regression, in the binary case, log loss is as follows:\n",
    "$$\n",
    "\\ell(h_{\\mathbf{w}}, (\\mathbf{x}, y)) = \\log(1 + \\exp(-y \\langle \\mathbf{w}, \\mathbf{x} \\rangle))\n",
    "$$\n",
    "\n",
    "This loss function penalizes the degree of wrongness in the case of misclassification.\n",
    "\n",
    "Log loss is also convex, which moves us onto the optimization of the loss function. The optimization is done according to empirical risk minimization, which aims to find the hypothesis within the hypothesis class that minimizes the expected loss over all available data. In other words, ERM selects the hypothesis that produces the lowest average loss on the entire dataset. Since log-loss is convex, it is known that there is at most one global minimum, which would be where the loss is the smallest. \n",
    "In order to find this minimima, the gradients of $L(w)$ are computed with respect to each weight $w_j$. This method called gradient descent is used to iteratively minimize the loss function by adjusting the model parameters in the direction that reduces loss. Specifically, the weights are updated iteratively as follows:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$\\alpha$ in this equation is the learning rate, which controls the size of the steps taken during gradient descent to update model parameters. It is important to select this parameter carefully because an overy large $\\alpha$ can cause the model to overshoot the optimal values, while an overly small $\\alpha$ can result in slow convergence or getting stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24efd6c-d83a-4e0a-9ba5-1e07b6b156af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
